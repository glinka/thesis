\chapter{Conclusion and discussion \label{ch:conc}}

Each of the chapters above has presented a novel application of
manifold learning, first reducing the dimensionality of network
systems and then identifying important parameters in generic
models. In both cases we have adapted the manifold learning technique
Diffusion Maps to the problem at hand. We have also employed Equation
Free modeling to both accelerate the simulation of a multigraph system
and to implement a stationary-state solver.

Chapter~\ref{ch:graphs} began with an introduction of the dynamic
multigraph model developed in
\cite{rath_multigraph_2012,rath_time_2012}. After identifying that the
degree distribution was a suitable coarse variable for the system, and
further reducing the low-dimensional description through curve
fitting, the equation free framework was successfully applied. This
not only accelerated simulations through coarse projective
integration, but also allowed us to locate stationary states through
coarse Newton-GMRES. Additionally, an underlying two-dimensional
description was uncovered using both principal component analysis and
Diffusion Maps, after a distance between networks was defined. These
automated methods of dimensionality-reduction are quite general, and
can in principle be applied in a wide range of network settings to
probe hidden low-dimensional structure in network models. The next
step in this direction is to combine the equation free method with
DMAPS output. This would algorithmically determine the low-dimensional
state space needed for equation free techniques, instead of relying on
domain knowledge and experimentation.

In Chapter~\ref{ch:sis} we continued this sort of analysis on an
epidemiological model, the susceptible-infected-susceptible
system. We again showed the effectiveness of applying DMAPS to obtain
reduced descriptions of collections of networks. Here we were able to
go one step further and relate the DMAPS output to system
variables. This investigation could be extended by comparing the
effectiveness of other network-similarity measures. While those used
in our work were successful, other measures may be faster to
compute. Chapters~\ref{ch:graphs} and ~\ref{ch:sis} extend previous
work and reaffirm the utility of DMAPS in the context of evolving
networks.

Finally, Chapter~\ref{ch:params} introduced a new variant of DMAPS
capable of identifying important parameter combinations in complex
models. By forming a kernel based on the model response at different
parameter values, DMAPS learns a parameterization of the model
manifold. In particular, the top eigenvectors should parameterize the
directions in which the manifold extends furthest. These, in turn,
correspond to the most significant parameter combinations. Thus, DMAPS
is able to parameterize the most important, nonlinear directions in
the input parameter space. The analysis does not resort to linear
approximations, nor does it require formal derivations of the reduced
parameter set. The method is model-agnostic by design, and thus can be
quickly deployed by researchers to probe their models' parameter space
for potential dimensionality reduction. This opens many avenues to
explore in the future. First, the DMAPS output can be used to guide
optimization routines along significant directions, avoiding sloppy
parameters that would otherwise slow convergence. Second, a more
sophisticated method of sampling the model manifold could be
developed. A technique involving geometric harmonics, similar to that
found in \cite{chiavazzo_intrinsic_2017} may work here. Finally,
investigating the relation between recent work in alternating
Diffusion Maps, \cite{lederman_learning_2015}, and our model-manifold kernel may
enable multi-objective optimization in the manner of~\cite{marler_survey_2004}.

While the work presented here advances effective data analysis of
networks and highly parameterized models, there are also many
interesting directions to pursue in the future. Dimensionality
reduction will continue to play a key role in the effort to better
understand data of all forms.



%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "../../thesis.tex"  ***
%%% End: ***