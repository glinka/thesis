\chapter{Introduction\label{ch:intro}}

Since the advent of the information age with the completion of the
first transistor-based computer in 1954, Moore's law has ensured a
steady decrease in the cost of computation. The concomitant rise in
simulation complexity paired with the proliferation of electronic
sensors has resulted in a torrent of data; the CERN Data Centre alone
proccesses around one thousand terabytes daily. To address the
challenge of separating the valuable bits from the uninformative, a
host of machine learning techniques have been developed, ranging from
the simplistic but venerable principal component analysis to complex
neural networks. Their successes are numerous. Principal component
analysis has been used to analyze PET scans \cite{friston_functional_1993}, to predict traffice flow \cite{zhang_forecasting_2007}, and to examine groundwater composition
\cite{helena_temporal_2000}. Neural networks can automatically segment images and
classify videos \cite{karpathy_large-scale_2014,zheng_conditional_2015}, and logistic regression has been used to
detect gene interactions \cite{park_penalized_2008}. These are but a few of the thousands
of successful applications of machine learning algorithms to large
datasets.

Despite this progress, the problem demands continuous innovation to
match the growing variety and supply of data. Dimensionality
reduction, the problem of describing a set of data in the fewest
number of variables possible without discarding important information,
is an increasingly imporant area of investigation. Dimensionality
reduction can reveal simpler ways of viewing a system, directing
researcher's attention to the few significant variables hidden in
their experimental output. Alternatively, it is often used as a
crucial pre-processing step when implementing machine learning models,
accelerating optimization. The work presented in this thesis focuses
on advances of the former type, where the algorithms we develop yield
insight into the important factors driving a system. In particular we
target the fields of complex networks and parameter estimation.

Defined as a set of items and the connections between them \cite{newman_structure_2003}, networks provide a
useful framework for modeling many real-world systems. In biology,
protein-protein interaction networks have been used to identify
distinct collections of proteins that are conserved across species
\cite{wuchty_evolutionary_2003}. Economist have constructed network
models of trade and determined that highly connected networks are more
sensititve to price variations \cite{li_spatial_2008}. Applications can even be found in
linguistics, where semantic network analysis indicates that
well-structured speeches win presidential debates \cite{Semantic
  networks and competition: Election year winners and losers in
  U.S. televised presidential debates, 1960â€“2004} (note that this
study included no data from 2016).

Regardless of the specific system, researchers often seek an
understanding of some emergent, macroscopic behavior. In
epidiemological models composed of thousands of interacting
individuals, often just the overall fraction that is infected is of
interest \cite{shi_sis_2008}. Ideally an analytical equation governing
the evolution of the quantity of interest would be readily derived
from a description of the fine-scale dynamics; however, as model
complexity increases in an effort to better match real-world
conditions, deriving such closed-form expressions becomes increasingly
difficult. In such cases, equation free modeling may enable the
desired macroscopic analysis despite the lack of explicit
equations. This approach has been successfuly applied to simple models
of network evolution \cite{bold_equation-free_2014}, and we extend
this work to hypergraph models in which multiple connections are
permitted between nodes.

Often the goal is even more basic: to determine whether a
low-dimensional description of the model exists in the first place. In
many models of network generation, the parameters that influence the
network structure are clear, see for instance
\cite{erdos_random_1959,chung_connected_2002,watts_collective_1998,
  Barabasi}. In more complicated models the picture is muddier, and it
would helpful to have an automated method of identifying whether such
a simpler description is present. Towards this end, we extend the
dimensionality-reduction technique Diffusion Maps to parameterize
collections of hypergraphs. This requires a suitable measure of the
similarity between two networks, a problem which is discussed in some
depth.

Finally, whether the system of interest is couched in terms of
networks, differential equations, or otherwise, fitting model
parameters to data is a nearly universal pursuit. It is often
discovered that certain parameter combinations do not significantly
affect the model output despite being varied over many orders of
magnitude. Such unimportant parameter directions are termed
``sloppy'', and cause fitting routines to converge slowly in addition
to obscuring otherwise-relevant parameters from investigators. Current
methods resort to linear analysis or require analytical derivations
of simpler models based on intimate knowledge of the system
\cite{Sensitivity Analysis;,transtrum_model_2014}. We again turn to Diffusion Maps to
address this problem, adapting it to parameterize the importance
parameter combinations of a model without knowledge of the explicit
model structure.

In total, it is my hope that the following chapters will be viewed as
a small advance in the larger march towards effective data
analysis. The methods are developed to be broadly applicable across
domains, and should arm researchers with tools that can quickly lead
to a deeper understanding of the system under investigation.


\input{ch-intro/tex/sub-outline}
\input{ch-intro/tex/sub-ml}
\input{ch-intro/tex/sub-cg}


% It is my hope that the
% work presented in this thesis will be viewed as a small advance in
% this larger march. 

% % % % possible first sentences of thesis

% The following chapters are a test of the theory that engineering
% theses are never read, neither by their advisor nor original
% author. Instead, the theory asserts that the author vomits the
% document into existence in a violent storm of anxiety and
% anticipation, never to be perused again.

% There are those that do brave battle with the big data and there are
% those that survey the wasted battlefield, strewn with broken
% algorithms and bad statistics, and run towards the siren sound of
% Excel. But there is no escape. Some of the warrior's
% deeds were recorded so that future generations would know the folly of
% csv files; the following documents were recovered from the battlefield
% of nonlinearities. Let them be a lesson to all.

% It is not uncommon for the modern researcher to pore over his
% gigabytes of data and question what it is all for.

% Should your data remain big for over six hours after reading this
% thesis, please consult your nearest graduate student.

% Modern researchers are often confronted by the unnecessarry bigness
% of their datasets and wish to make them smaller. This can occur for a
% variety of reasons. 

% Despite growing demand for algorithms that work good, scientists
% are unable to make them good.

% It is no secret that big data requires big algorithms.

%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "../../thesis.tex"  ***
%%% End: ***